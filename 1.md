### 第17章 提示词生成应用：从零训练模型

前面所讲的内容中直接采用了预训练模型或在预训练模型上进行微调后投入使用，这些模型的参数量已经达到很高的规模，模型储备了大量的知识，可以实现强大的功能。在学习这些内容的过程中，读者很可能会产生一些疑问：这些预训练模型是怎么形成的？我们可否在较低的算力条件下自己训练一个小规模的预训练模型？ 

本章主要讲解如何从零训练一个模型，并将其应用于提示词的生成场景中。本章的开发工作涉及大语言模型训练的大部分流程，包括语料的整理、词汇表的生成、语料向量化、训练和应用环节。

#### 17.1 目标

有别于之前直接使用或微调一个预训练模型，本章使用GPT-2技术从零开始训练一个提示词生成模型，用较少的语料、较低的算力，在较短的时间内训练一个规模较小、场景范围限定的预训练模型。过程中涉及语料整理、词汇表生成、向量化、训练和接口应用等环节，都用Python程序实现。经过本章的实践，读者可以学习到大语言模型训练的基本原理和大部分流程。 

本应用的名称是prompt-gen，意为“提示词生成器”，即在专业领域的语料上训练得到一个模型，并在此基础上开发一个用于生成提示词的应用。用户输入提示词的前几个字，该应用就能预测并生成后续的提示词句子，并可一次生成多个候选项。其使用场景类似于搜索引擎输入框或者汉字输入法工具，区别在于该应用提供的候选项是句子而非词或短语。

#### 17.2 原理
##### 17.2.1 GPT-2简介

1. **GPT-2简介**

GPT-2是OpenAI开发的一种基于Transformer架构的自然语言处理模型，是GPT系列模型的第二代，于2019年发布。由于大语言模型发展迅速，进化非常快，短短几年间GPT经历了几个大版本的迭代，为业界带来了巨大的技术冲击，如GPT-2具有强大的文本生成能力，GPT-3应用于Github Copilot协助开发者编程，ChatGPT基于模型技术GPT-3.5实现，多用途的GPT-4目前正在流行。相对于GPT-2，GPT-4在参数规模、功能、性能等方面，都拉开相当大的差距。但从工作原理方面来看，两者都采用了Decoder-only的Transformer架构，采用预训练、无监督学习等技术。而经过了这几年的发展，GPT-4在架构上进行了优化，对Transformer模型进行了改进或变种，在更大规模、更多样化的数据集上进行了训练，从而学习到更丰富的语言表示，其参数规模更大，能够捕捉更复杂的语言模式和关联，在生成文本的质量、理解能力上实现了显著提升。 

从大语言模型学习的角度来考虑，GPT-2代表了自然语言处理领域的一次重要突破，为后续模型的发展铺平了道路。GPT后续版本的技术体系与GPT-2有很多相似性，所以学习GPT-2模型训练过程，也会对GPT系列其他产品的运行原理具有一定的了解。 

从应用的角度来考虑，GPT-2在生成文本时表现得流畅自然，可以产生连贯的文章、故事和代码，其参数规模小、训练周期短、运行效率高，用途很广，还能应用于某些特定的垂直领域，比如医疗文书的推理生成、专用搜索的提示词生成、专业知识问答等。 

从大语言模型的技术实践来考虑，用已有的大语言模型进行二次训练叫“微调”，用GPT-2从零训练一个模型才叫“训练”。训练产生预训练模型，微调在预训练模型上进行，两者有本质的区别。掌握了训练过程和方法，对微调的原理和过程的认识就会更为清晰。


2. **GPT-2的特点** 

GPT-2是OpenAI最后一个开源GPT版本，后续的GPT-3、GPT-3.5、GPT-4都不再开源。正因为如此，开发者可以基于Transformers库封装的GPT-2相关代码训练全新的预训练模型，而不是用预训练模型微调。由于GPT-2出现较早，以GPT-2为基础训练一个通用大模型的成本非常高，效果也不会太好。但作为一个应用于垂直领域的底座模型及构建推理补全应用的技术体系，GPT-2完全可以胜任。 

GPT-2基于Transformer架构实现，通过在大规模文本数据上进行自监督学习，可以学习到丰富的语言表示。在预训练阶段，模型通过“阅读”大量文本数据来学习语言的内在结构和规律，从而为后续的微调任务提供良好的基础。这种预训练方法使得模型能够更快速地适应特定任务的需求，提高了模型的泛化能力和适应性。 

作为一种无监督学习模型，GPT-2可以在没有人工标注数据的情况下进行训练，从而避免了标注大量数据的繁重工作。无监督学习使得模型更具通用性和灵活性，可以适用于各种自然语言处理任务，为模型的应用范围和潜力带来了显著的扩展。 

除了文本生成外，GPT-2还可应用于机器翻译、情感分析等多种自然语言处理任务。这种通用性使得GPT-2成为一个“多才多艺”的模型，能够适应不同领域和任务的需求，为自然语言处理领域的研究和应用提供了丰富的可能性。

##### 17.2.2 训练流程与应用架构

prompt-gen是一个从模型训练到推理的应用，其开发过程基本涵盖了大语言模型文本应用的大部分技术环节。流程中各个环节之间的关系见图17-1。

![image](https://github.com/user-attachments/assets/8f56217f-a83d-4c95-a81e-5fdc201ea996)


**图17-1 prompt-gen模型训练流程与应用架构图**

其主要流程如下。

①原始语料收集：采集来的语料整理成TXT格式文件。

②语料JSON格式转换：将TXT格式转换成JSON格式，便于后续处理。 

③词汇表生成：从语料JSON文件中计算生成词汇列表文件。 

④Tokenization：使用词汇表中的词汇和索引位置，将语料JSON的内容分隔成若干份由“token id”组成的数字表示文件。 

⑤训练过程：初始化模型参数，装载tokenized文件，经过前向推理计算损失函数、反向传播、计算当前梯度和根据梯度更新网络参数等过程，训练生成模型。 

⑥推理过程：输入上下文本，模型推理生成token序列，再由词汇表翻译成预测目标明文。

##### 17.2.3 训练方法与运行原理

1. **语料的收集** 

大语言模型的语料收集过程是一个关键的步骤，直接影响模型的预训练效果和性能。在收集语料时，需要考虑数据的质量、多样性和覆盖范围，以确保模型能够学习到丰富和具有代表性的语言知识。 

为了确保模型能够正确理解和处理文本数据，需要对数据格式进行标准化处理。这包括统一的文本编码格式、句子分隔符、标点符号等，以便模型能够准确地识别句子和段落的边界。 

对于特定用途、规模较小的模型，语料的来源比较单一，一般来自专业领域的文章或结构化数据。首先将这些原始语料整理成TXT文件。在提示词生成的场景中，主要考虑段落、句子内部单词之间的逻辑结构，所以对原始文本语料的处理较为粗放，以“行”为单位，每行文本作为一个JSON元素，组织成一个大的数组，完成语料的初步收集。 

例如，以下为TXT格式的原始语料。 
```js
建议化验痰培养，以及肺炎支原体抗体，明确致病原因后，针对性选择药物 
需要排除其他原因引起的咳嗽，比如过敏性咳嗽 
对于慢性咳嗽疾病的出现，应该做到早发现早治疗 
因为早期的慢性咳嗽是容易得到控制的 
患者们不要错过治疗的好时机 
将上述语料整理成JSON数组，格式如下。
```
```
[ 
 "建议化验痰培养，以及肺炎支原体抗体，明确致病原因后，针对性选择药物", 
 "需要排除其他原因引起的咳嗽，比如过敏性咳嗽", 
 "对于慢性咳嗽疾病的出现，应该做到早发现早治疗", 
 "因为早期的慢性咳嗽是容易得到控制的", 
 "患者们不要错过治疗的好时机" 
]
```

**注** 每行文本在TXT文件中是“行”，在JSON文件中依然是“行”，本质上没有区别，只是为方便后续处理而进行了格式转换。

2. **词汇表生成** 

在GPT中，词汇表（Vocabulary）的作用是将文本数据中的单词映射为模型可以理解和处理的数字表示，包含了模型所能识别的大部分单词及其对应的索引。出于性能上的考虑，词汇表中一般不会包括所有的词，而且词汇表来源于原始语料。 

词汇表中的每个单词都会被映射到一个唯一的索引，这样模型可以通过索引来表示和处理单词。词汇表中的单词索引用于查找对应的词嵌入（Word Embedding）。词嵌入是将单 


![image](https://github.com/user-attachments/assets/e259a0ef-49a7-4a65-bc7d-ca539e383c43)

![image](https://github.com/user-attachments/assets/d2b022bd-6b71-41dc-ac47-6be21b7d361b)

