
语言问题就这样变成了数字计算问题。

①初始化模型参数：从给定的配置文件中加载模型配置参数，初始化GPT - 2模型，既可以从零开始训练，也可以从上次训练的检查点开始训练（可以理解为装载预训练模型来进行继续训练）。

②初始化训练变量：设定一些训练过程中需要使用的变量，如n_ctx（上下文长度）、tb_writer（用于记录训练日志）、multi_gpu（是否使用多GPU训练）、full_len（数据集总长度）、overall_step（总步数）、running_loss（运行损失）等。

③计算总步数：根据数据集长度、训练轮数、批大小等参数计算总步数。

④初始化优化器和学习率调度器：使用AdamW优化器和WarmupLinearSchedule学习率调度器进行模型优化与学习率调整。如果启用了混合精度训练（FP16），则使用apex库进行初始化。

⑤多GPU支持：如果检测到有多个GPU可用，则将模型放到多个GPU上进行训练。

⑥训练模型：循环每个训练轮次，遍历所有数据分片，准备数据并进行前向推理、计算损失函数、反向传播、更新网络参数等操作，在每个训练轮次结束后保存模型检查点。

⑦保存最终模型：完成所有轮次后，模型训练完成，保存最终的模型。

### 5. 推理

模型训练完成后，就可以使用模型进行预测了。给定上文，让模型预测出下文的候选结果，具体步骤如下。

①构造上下文管理器，关闭PyTorch的梯度追踪功能，在生成文本时不需要计算梯度。

②使用预训练的模型对给定的输入进行预测，同时传入历史信息。

③从模型输出中提取出预测和更新后的历史信息，以备下次预测时使用。


④从模型输出中取出最后一个输出的token，并进行温度缩放，以控制生成文本的多样性。

⑤对经过温度缩放后的输出进行top - k和top - p过滤，得到过滤后的对数概率logits。

⑥根据过滤后的logits，使用抽样方法从softmax分布中采样一个token。

⑦将采样得到的token添加到文本生成序列中。

⑧将采样得到的token作为下一个输入，继续生成下一个token。

⑨通过循环逐步生成token序列，并在每一次循环中根据模型输出和采样策略得到下一个token，直到生成指定长度的token序列为止。

⑩此时生成的token序列还不是明文，需要用分词标记器tokenizer将序列中的每个token id对应转化为词汇表中的词汇，最后将这些词汇连成句子。考虑到中文和英文的差异，英文单词前后要加空格。

## 6. 接口服务

采用aiohttp的异步Web服务，对预训练模型文本生成功能进行封装。客户端发送HTTP的POST请求，服务端监听到请求后，调用模型生成候选项列表，返回给客户端。

例如，客户端发送的请求如下。
```
{"context":"你好","maxlength":50,"samples":5}
```

该请求表示，在本次生成任务中，上文是“你好”，生成的最大长度为50，生成5个候选项。

服务端接收到此请求后，解析出context、maxlength和samples三个参数，调用模型生成推理的结果后返回给客户端。
```
["你好！对于你这种情况，……",
"你好：根据您的描述，您的甲状腺……",
"你好，这种情况首先要排除……",
"你好，情况呢……",
"你好，这是因为……"]
```
# 17.3 开发与训练过程
GPT - 2技术出现较早，在GitHub上有大量的成熟代码可供参考，其中GPT2 - Chinese获得了7.3K的Star，研究GPT2 - Chinese的代码对于学习GPT系列模型大有益处。本章的训练代码参考了GPT2 - Chinese的实现过程，并进行了参数简化、代码职责分离和功能扩充等方面的优化。下面对GPT - 2的完整训练过程进行分步介绍，便于读者学习。其完整代码见https://github.com/little51/llm - dev/tree/main/chapter17。

https://github.com/Morizeyao/GPT2 - Chinese。

186 开发篇

## 17.3.1 语料整理
### 1. 语料整理环境建立

语料的建立过程是对语料进行整理、格式转换和从中分析出大部分词汇的过程，其中用到了thulac分词工具和Keras字处理工具。thulac（THU Lexical Analyzer for Chinese）是一个由清华大学研发推出的一套中文词法分析工具包，具有中文分词和词性标注功能。Keras是一个用Python编写的高级神经网络API，依赖于Google的TensorFlow机器学习框架。其组件keras.preprocessing.text是一个文本预处理工具类，该类允许使用两种方法向量化一个文本语料库：一种是将每个文本转化为一个整数序列（每个整数都是词汇表中标记的索引）；另一种是将其转化为一个向量。其中每个标记的系数可以是二进制值、词频、TF - IDF权重等。

首先，建立一个测试目录。
```
mkdir prompt - gen
cd prompt - gen
```
然后，建立一个Python3.10虚拟环境，名为prompt - vocab，命令如下。
```
conda create -n prompt - vocab python = 3.10 -y
conda activate prompt - vocab
```
接着，新建一个库依赖列表文件，名为requirements - vocab.txt，内容如下。
```
tqdm==4.66.2
thulac==0.2.2
tensorflow==2.15.0
keras==2.15.0
```
最后，按照依赖关系安装依赖库，命令如下。
```
pip install -r requirements - vocab.txt -i \
https://pypi.mirrors.ustc.edu.cn/simple \
--trusted - host = pypi.mirrors.ustc.edu.cn
```
### 2. 语料JSON格式生成
对收集到的原始语料进行简单处理，保存为TXT格式的文件，并存放到data目录下。编写格式转换程序build_trainfile.py，将TXT语料转换成JSON格式的文件——data/train.json。build_trainfile.py的源码如下。
```
import json
import glob
if __name__ == "__main__":
    names = glob.glob('./data/*.txt')
    train_data = []
    x = 0
    for j, name in enumerate(names):
        f = open(name, 'r+', encoding='utf - 8')
        lines = [line for line in f.readlines()]
        for i in range(len(lines)):
            text = lines[i].strip()
            train_data.append(text)
        f.close()
        x = x + 1
        print(str(x) + " of " + str(len(names)))
    with open('./data/train.json', 'w+', encoding='utf - 8') as f:
        json.dump(train_data, f, ensure_ascii=False)
````
运行以下命令，生成目标文件data/train.json。
```
conda activate prompt - vocab
# 将data/*.txt生成train.json
python build_trainfile.py
```
### 3. 词汇表生成
词汇表生成过程的输入是语料文件data/train.json，输出是词汇表文件vocab/vocab_user.txt。该处理过程用到了中文分词和文本序列化，默认生成50000个词，再加上5个特殊词汇，一共50005个。make - vocab.py的完整代码如下。
```
import argparse
import thulac
import json
from tqdm import tqdm
from keras.preprocessing.text import Tokenizer
def make_vocab(raw_data_path, vocab_file, vocab_size):
    lac = thulac(seg_only=True)
    tokenizer = Tokenizer(num_words=vocab_size)
    with open(raw_data_path, 'r', encoding='utf - 8') as f:
        lines = json.load(f)
        for i, line in enumerate(tqdm(lines)):
            try:
                lines[i] = lac.cut(line, text=True)
            except:
                lines[i] = line
    tokenizer.fit_on_texts(lines)
    vocab = list(tokenizer.index_word.values())
    pre = ['[SEP]', '[CLS]', '[MASK]', '[PAD]', '[UNK]']
    vocab = pre + vocab
    with open(vocab_file, 'w', encoding='utf - 8') as f:
        for word in vocab[:vocab_size + 5]:
            f.write(word + '\n')
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--raw_data_path',
                        default='./data/train.json',
                        type=str, required=False)
    parser.add_argument('--vocab_file',
                        default='./vocab/vocab_user.txt',
                        type=str, required=False)
    parser.add_argument('--vocab_size', default=50000,
                        type=int, required=False)
    args = parser.parse_args()
    make_vocab(args.raw_data_path, args.vocab_file, args.vocab_size)
```
运行以下命令，生成词汇表文件。
```
conda activate prompt - vocab
# 按train.json生成词汇表
python make - vocab.py
```
## 17.3.2 训练
### 1. 训练环境建立
语料处理过程依赖的TensorFlow与训练过程keras3.1.1之间有依赖冲突，所以分别建立两个Python虚拟环境以进行隔离：一个是prompt - vocab，用于词汇表生成；另一个是prompt - train，用于训练。其实现过程如下。
首先，建立一个Python3.10虚拟环境，名为prompt - train，命令如下。
```
conda deactivate
conda create -n prompt - train python = 3.10 -y
conda activate prompt - train
```
然后，新建一个库依赖列表文件，名为requirements - train.txt，内容如下。
```
transformers==2.1.1
torch==2.0.1
numpy==1.26.4
tqdm==4.66.2
scikit - learn==1.4.1.post1
future==1.0.0
thulac==0.2.2
aiohttp_cors==0.7.0
keras==3.1.1
tensorboard==2.16.2
packaging==24.0
```
最后，按照依赖关系安装依赖库，命令如下。
```
pip install -r requirements - train.txt -i \
https://pypi.mirrors.ustc.edu.cn/simple \
--trusted - host = pypi.mirrors.ustc.edu.cn
```
![image](https://github.com/user-attachments/assets/c6f58899-e754-402d-b2f1-340d3a73dd89)

![image](https://github.com/user-attachments/assets/5b2e9b3d-1bfa-4821-bacf-137998374b2c)

```
    with open(tokenized_data_path +
              '/tokenized_train_{}.txt'.format(i), 'w') as f:
        for id in full_line:
            f.write(str(id) +'')

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--raw_data_path',
                        default='./data/train.json',
                        type=str, required=False)
    parser.add_argument('--tokenized_data_path',
                        default='./tokenized',
                        type=str, required=False)
    parser.add_argument('--vocab_filename',
                        default='./vocab/vocab_user.txt',
                        type=str, required=False)
    parser.add_argument('--num_pieces', default=100, type=int,
                        required=False)
    parser.add_argument('--min_length', default=128, type=int,
                        required=False)
    args = parser.parse_args()
    make_tokenized(args.raw_data_path, args.tokenized_data_path,
                   args.vocab_filename, args.num_pieces,
                   args.min_length)
```
运行以下命令，生成tokenized文件。而有num_pieces个目标文件将在tokenized文件夹下生成。
```
conda activate prompt - train
python make - tokenized.py
```
### 3. 训练过程
模型训练过程相对于其他步骤来说较为复杂，可以细分为依赖库导入、初始化模型参数、初始化变量、计算总步数、FP16支持、多GPU支持、训练计算等步骤，下面分别介绍。
#### （1）依赖库导入
模型训练过程中主要用到了Transformers、PyTorch、NumPy等依赖库。
```python
import transformers
import torch
import os
import json
import random
import numpy as np
import argparse
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
from tqdm import tqdm
from torch.nn import DataParallel
```
#### （2）初始化模型参数
按model_config设定的模型参数文件来初始化模型，主要参数如下。
```json
{
    "initializer_range": 0.02, # 初始化模型参数范围，影响模型收敛速度和性能
    "layer_norm_epsilon": 1e - 05, # 用于避免除零错误和确保数值稳定性
    "n_ctx": 1024, # 模型在处理输入时所支持的最大token长度
    "n_embd": 768, # 模型中词嵌入的维度，值越大，生成的文本越丰富和准确
    "n_head": 12, # 注意力头的数量，影响模型的并行性和表征能力
    "n_layer": 12, # Transformer中编码器和解码器的层数，较大的值通常可提高模型的性能和泛化能力
    "n_positions": 1024, # 模型能够处理的最大序列长度
    "vocab_size": 50005 # 词汇数量
}
```
其源程序如下。
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
def init_model(args):
    model_config = transformers.modeling_gpt2.GPT2Config.from_json_file(
        args.model_config
    )
    print('模型配置参数:\n' + model_config.to_json_string())
    if not args.pretrained_model:
        model = transformers.modeling_gpt2.GPT2LMHeadModel(
            config=model_config
        )
    else:
        model = transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained(
            args.pretrained_model
        )
    model.train()
    model.to(device)
    num_parameters = 0
    parameters = model.parameters()
    for parameter in parameters:
        num_parameters += parameter.numel()
    print('参数个数: {}'.format(num_parameters))
    return model, model_config
```
#### （3）初始化变量
定义训练时会用到一些变量的初始值，如下所示。
```python
def init_variable(args, model_config):
    n_ctx = model_config.n_ctx
    tb_writer = SummaryWriter(log_dir=args.writer_dir)
    multi_gpu = False
    full_len = 0
    overall_step = 0
    running_loss = 0
    return n_ctx, tb_writer, multi_gpu, full_len, \
           overall_step, running_loss
``` 
