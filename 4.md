

### （8）模型训练
执行以下命令开始训练。
```bash
conda activate prompt - train
python train.py
```
如果发生内存溢出，则可适当降低模型配置文件中的n_ctx、n_head、n_layer等参数值。训练过程见图17 - 2。

![GPT - 2模型训练过程](图17 - 2位置，此处无实际图片链接，原书有图)

![image](https://github.com/user-attachments/assets/489d7547-e24f-459e-b60f-f16dd214e964)


### 17.3.3 推理与服务
每轮训练完成后保存一个模型检查点。如果还有空闲算力资源，就可装载这个模型进行测试评估，或等全部轮次完成、算力资源释放后再进行验证。GPT - 2的技术出现相对较早，本例中，为了兼容BERT相关库，Transformers库使用较早的2.1.1版本。在对GPT - 2模型进行推理时，代码比较烦琐，但是读者可以更好地理解大语言模型推理的工作原理。
### 1. 推理过程
对推理过程的代码文件generate.py进行适当拆解，分成依赖库导入、模型装载、token预测、token过滤、token解析、程序入口及推理测试几个环节，依次进行源码介绍。
#### （1）依赖库导入
导入依赖的Transformers库里的GPT - 2模型和Tokenizer、PyTorch和BERT等相关库。
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
from tqdm import trange
from tokenizations.bert import tokenization_bert_word_level\
    as tokenization_bert
```
#### （2）模型装载
从训练输入的位置装载预训练模型，而tokenizer则由BertTokenizer根据词汇表文件反向生成。
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = None
tokenizer = None
def load_model():
    global model
    global tokenizer
    model = GPT2LMHeadModel.from_pretrained(
        "./model/final_model", torchscript=True).eval().to(device)
    tokenizer = tokenization_bert.BertTokenizer(
        vocab_file="./vocab/vocab_user.txt")
```
#### （3）token预测
给定初始文本，调用模型，生成一段指定长度的文本序列。
```python
def fast_sample_sequence(model, raw_text, length,
                         temperature=1.0, top_k=30, top_p=0.0):
    # 将输入的原始文本转换为token id列表
    context = tokenizer.convert_tokens_to_ids(
        tokenizer.tokenize(raw_text))
    # 将token id列表转换为LongTensor，并调整形状为[1, seq_length]
    inputs = torch.LongTensor(context).view(1, -1).to(device)
    if len(context) > 1:
        # 如果context的长度大于1，则使用model对前面个数为seq_length - 1的token进行预测，得到过去信息past
        past = model(inputs[:, :-1], None)[2]
        prev = inputs[:, -1].view(1, -1)
    else:
        # 否则，将past设为None，并将prev设为整个输入序列
        past = None
        prev = inputs
    # 初始化generate列表为输入文本的token id列表
    generate = [] + context
    # 使用torch.no_grad()上下文管理器，关闭梯度计算
    with torch.no_grad():
        for i in trange(length):
            # 使用model预测下一个token，并更新过去信息past
            output = model(prev, past=past)
            output = output[0][:, -1].squeeze(0) / temperature
            # 将输出进行温度缩放
            # 然后通过top_k_top_p_filtering函数过滤logits
            filtered_logits = top_k_top_p_filtering(
                output, top_k=top_k, top_p=top_p)
            # 根据过滤后的logits，从softmax分布中采样一个token
            next_token = torch.multinomial(torch.softmax(
                filtered_logits, dim=-1), num_samples=1)
            # 将采样得到的token添加到generate列表中
            # 继续下一个token的生成
            generate.append(next_token.item())
            prev = next_token.view(1, 1)
    # 返回生成的文本序列
    # 其中包含了原始文本的内容和模型生成的文本内容
    return generate
```
#### （4）token过滤
按照top_k和top_p参数，从生成的token序列中过滤掉概率较低的token。
```python
def top_k_top_p_filtering(logits, top_k=0, top_p=0.0,
                          filter_value=-float('Inf')):
    # 确保对数概率logits为1，即一维张量
    assert logits.dim() == 1
    # 将top_k限制在logits最后一个维度的大小以内
    top_k = min(top_k, logits.size(-1))
    if top_k > 0:
        # 对于概率小于前top_k个最大概率的token，找到其对应的索引
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        # 将这些token对应的logits值设为一个非常小的负无穷值filter_value
        logits[indices_to_remove] = filter_value
    if top_p > 0.0:
        # 对logits进行降序排序，得到排序后的概率值和对应的索引
        sorted_logits, sorted_indices = torch.sort(logits,
                                                   descending=True)
        # 计算排序后的累积概率分布
        cumulative_probs = torch.cumsum(
            F.softmax(sorted_logits, dim=-1), dim=-1)
        # 找到累积概率超过阈值top_p的token所对应的索引
        sorted_indices_to_remove = cumulative_probs > top_p
        # 对sorted_indices_to_remove进行处理
        sorted_indices_to_remove[...,
                                 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        # 根据索引找到需要移除的token
        indices_to_remove = sorted_indices[sorted_indices_to_remove]
        # 将这些token对应的logits值设为filter_value
        logits[indices_to_remove] = filter_value
    return logits
```
#### （5）token解析
模型推理生成的是token序列，也就是由token id组成的数组。该序列要经过词汇表的对应值翻译后，才可以还原成明文。
```python
def is_word(word):
    # 判断生成的token是否为英文
    for item in list(word):
        if item not in 'qwertyuiopasdfghjklzxcvbnm':
            return False
    return True


def join_text(texts):
    # 把生成的token数组连接起来形成句子
    for i, item in enumerate(texts[:-1]):  # 确保英文前后有空格
        if is_word(item) and is_word(texts[i + 1]):
            texts[i] = item + ' '
    for i, item in enumerate(texts):
        if item == '[MASK]':
            texts[i] = ' '
        elif item == '[CLS]':
            texts[i] = '\n\n'
        elif item == '[SEP]':
            texts[i] = '\n'
        elif item == '[UNK]':
            texts[i] = ' '
    return ''.join(texts).replace('##', '').strip()


def generate_text(prompt, max_len, batch_size):
    # 以prompt为开头，推理生成后续提示词文本
    if model is None:
        load_model()
    generates = []
    for i in range(batch_size):
        generate = fast_sample_sequence(model, prompt, max_len)
        texts = tokenizer.convert_ids_to_tokens(generate)
        generates.append(join_text(texts))
    return generates
```
#### （6）程序入口
下面代码展示了主函数，用于在命令行环境下进行测试，可实现用户输入上文、模型预测下文的功能，并且可以一次批量生成若干个候选项。
```python
if __name__ == "__main__":
    while True:
        prompt = input("请输入文本，回车退出: ")
        if prompt == "":
            break
        print(generate_text(prompt, 50, 5))
```
#### （7）推理测试
执行以下命令开始测试推理。代码运行结果见图17 - 3。
```bash
conda activate prompt - train
python generate.py
```

![prompt - gen推理过程测试](图17 - 3位置，此处无实际图片链接，原书有图)

![image](https://github.com/user-attachments/assets/562f878b-4f9d-44a8-8a4f-2b298818494a)


### 2. 接口服务
本例实践的是提示词生成器，在命令行下只适合测试模型的生成效果，而在生产场景中需要公布对外接口供客户端调用。源程序web.py实现了此功能。它引入aiohttp库实现了一个HTTP服务器，监听客户端发送的POST请求，调用大模型推理生成候选项组成的数组返回给客户端。以下是其完整源代码。
```python
import asyncio
import json
import time
import os
import aiohttp_cors
import requests
import argparse
from aiohttp import web
from generate import generate_text


async def generate(request):
    params = await request.json()
    context = params["context"]
    if len(context.strip()) == 0:
        return web.Response(
            content_type="application/json",
            text=json.dumps(
                {"result": "[]", "time": 0}
            ),
        )
    maxlength = params["maxlength"]
    samples = params["samples"]
    if samples == 0:
        samples = 1
    start = time.perf_counter()
    result = generate_text(context, maxlength, samples)
    end = time.perf_counter()
    return web.Response(
        content_type="application/json",
        text=json.dumps(
            {"result": result, "time": end - start}
        ),
    )


app = web.Application()
cors = aiohttp_cors.setup(app)
app.router.add_post("/generate", generate)
for route in list(app.router.routes()):
    cors.add(route, {
        "*": aiohttp_cors.ResourceOptions(
            allow_credentials=True,
            expose_headers="*",
            allow_headers="*",
            allow_methods="*"
        )
    })

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', default=5005,
                        type=int, required=False)
    args = parser.parse_args()
    print("Start web server")
    web.run_app(
        app, access_log=None, host="0.0.0.0",
        port=args.port,
        ssl_context=None
    )
```
执行以下命令开启Web服务接口，运行结果见图17 - 4。
```bash
conda activate prompt - train
python web.py
```

![prompt - gen应用的Web服务运行情况](图17 - 4位置，此处无实际图片链接，原书有图)

![image](https://github.com/user-attachments/assets/c1ed1f1f-dd24-45fb-9a6e-3782999dcd75)


### 17.3.4 测试

对于HTTP的POST请求，可以用curl命令测试。Linux自带curl工具，Windows要从https://curl.se/windows/下载。

对于curl命令测试，在Linux与Windows上，其参数中的引号格式有一定区别，如下所示。
```bash
# Windows
curl -X POST http://127.0.0.1:5005/generate -H "Content-Type: application/json" -d "{\"context\":\"高血压要注意\",\"maxlength\":50,\"samples\":5}"
# Linux
curl -X POST http://127.0.0.1:5005/generate -H "Content-Type: application/json" -d '{"context":"高血压要注意","maxlength":50,"samples":5}'
```

测试结果见图17 - 5。因为curl默认无法处理unicode显示，所以汉字都显示为“\uXXX”格式。

要将这些unicode还原成汉字，可将结果粘贴到unicode与中文互转在线工具中（如https://www.bejson.com/convert/unicode_chinese/）。如图17 - 6所示，测试结果正常显示为汉字。

在生产环境中，由于客户端通常使用JavaScript、Python等编程语言，而非curl命令行环境，所以结果中的汉字不用进行二次转码处理。

![image](https://github.com/user-attachments/assets/91a20f0e-2d2a-4b81-a911-631e41ac369b)

![image](https://github.com/user-attachments/assets/d0a3bf0a-53ed-4d77-abc5-494f09bcceba)



![curl测试Web服务](图17 - 5位置，此处无实际图片链接，原书有图)

![curl返回unicode转汉字](图17 - 6位置，此处无实际图片链接，原书有图) 
